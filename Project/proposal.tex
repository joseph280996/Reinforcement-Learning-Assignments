\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Reinforcement learning for raceline prediction}
\author{Joseph Hadidjojo, Tung Pham}
\date{\vspace{-1em}}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

% for final report only
%\begin{abstract}
%A one paragraph high level description of the work. The abstract is typically the first thing someone would read from the paper before deciding whether to continue reading and hence, serves as an advertisement to the reader to read the whole paper. 
%\end{abstract}

\section{Introduction}

The Formula 1 is a open-wheel single-seater formula racing cars. Its tournament,
the FIA Formula One World Championship, has been one of the world's premier forms of racing since 1950.
Participants are all top racers, however, the champions are those that talented
in choosing the correct racing line for a given track. This put them
leagues above the others. However, most of these are estimates based on the
experience of racers and it would be useful for them to be able to see the ideal
racing line during practice and training.

In this project, we're aiming to solve this problem with the help of
Reinforcement Learning techniques in order to help visualize the mistake that
racers made to allow better adjustment and practice session. Specifically, we'll
train an agent that can identify the optimal racing line given a track using
multiple on-policy and off-policy update methods. That way, we can highlight the
mistake that the racer made in comparison with the optimal racing line. We'll
also be developing multiple different reinforcement learning methods and
approaches. We'll compare the performance of each with a baseline model to see
the improvements and benefit.

\section{Background Related Work}

\section{Technical Approach}
\subsection{Environment Setup}
To simulate the racing environment, we're planning to use a simple, single agent
environment as the training ground. We'll assume that the track has an
idealistic surface, meaning that it is a flat surface, with no no elevation or
bumps, that can be represented
using a 2D matrix. We'll also ignore all types of friction like air friction and terrain. 
The width of racetrack is kept fixed throughout and the agent can only choose
the actions that select a fixed value for acceleration and deceleration of the
car at each time step. There is, however, a cap speed which limits how fast the
agent can go. All elements regarding the racecar like tire degradation, inertia, momentum,
etc. are also ignored. 

In the scenario that the agent failed to take a curve and is thrown off the
racetrack, the agent will continue from the nearest location on the racetrack in
the next timestep and the velocity as well as the acceleration is resetted to
zero. This will required the agents to start all over which will take longer to
complete the track if the agent was able to perform this curve correctly.

We'll be using the racing car environment that comes with Gymnasium, a
Python's library that comes with multiple predefined environment. There are two types of environments, a
discrete and continuous. In this project, we're mostly concern with discrete
environment and we will use it through out to test the performance of our models.
\subsection{Methodology}
\subsubsection{Q-Learning}

\subsubsection{SARSA}
\subsubsection{Deep Q-Network}
\subsubsection{Deep Q-Network}
\subsubsection{Deep Deterministic Q-Network}
\subsubsection{Proximal Policy Optimization}
\section{Evaluation}
\section{Timeline and Individual Responsibilities}

\bibliographystyle{plain}
\bibliography{references}
\end{document}

